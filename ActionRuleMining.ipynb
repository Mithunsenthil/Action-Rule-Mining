{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install category_encoders\n",
        "!pip install ucimlrepo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ov40KgLwPY1T",
        "outputId": "73249850-6006-4f46-d644-fb1a5a7ea4c2"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: category_encoders in /usr/local/lib/python3.11/dist-packages (2.8.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (2.2.2)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (1.0.1)\n",
            "Requirement already satisfied: scikit-learn>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (1.13.1)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (0.14.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->category_encoders) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->category_encoders) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->category_encoders) (2025.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.6.0->category_encoders) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.6.0->category_encoders) (3.5.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.9.0->category_encoders) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->category_encoders) (1.17.0)\n",
            "Requirement already satisfied: ucimlrepo in /usr/local/lib/python3.11/dist-packages (0.0.7)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ucimlrepo) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2020.12.5 in /usr/local/lib/python3.11/dist-packages (from ucimlrepo) (2025.1.31)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "y3K6UdzVOvVg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.tree import DecisionTreeClassifier, _tree\n",
        "import category_encoders as ce"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ucimlrepo import fetch_ucirepo\n",
        "import pandas as pd\n",
        "\n",
        "# fetch dataset\n",
        "credit_approval = fetch_ucirepo(id=27)\n",
        "\n",
        "# data (as pandas dataframes)\n",
        "X = credit_approval.data.features\n",
        "y = credit_approval.data.targets\n",
        "\n",
        "# metadata\n",
        "print(credit_approval.metadata)\n",
        "\n",
        "# variable information\n",
        "print(credit_approval.variables)\n",
        "\n",
        "credit_approval= pd.concat([X, y], axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixH0O5LAQEEw",
        "outputId": "24936c32-e2a6-4c88-a0cf-fd38a7e6e911"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'uci_id': 27, 'name': 'Credit Approval', 'repository_url': 'https://archive.ics.uci.edu/dataset/27/credit+approval', 'data_url': 'https://archive.ics.uci.edu/static/public/27/data.csv', 'abstract': 'This data concerns credit card applications; good mix of attributes', 'area': 'Business', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 690, 'num_features': 15, 'feature_types': ['Categorical', 'Integer', 'Real'], 'demographics': [], 'target_col': ['A16'], 'index_col': None, 'has_missing_values': 'yes', 'missing_values_symbol': 'NaN', 'year_of_dataset_creation': 1987, 'last_updated': 'Wed Aug 23 2023', 'dataset_doi': '10.24432/C5FS30', 'creators': ['J. R. Quinlan'], 'intro_paper': None, 'additional_info': {'summary': 'This file concerns credit card applications.  All attribute names and values have been changed to meaningless symbols to protect confidentiality of the data.\\r\\n  \\r\\nThis dataset is interesting because there is a good mix of attributes -- continuous, nominal with small numbers of values, and nominal with larger numbers of values.  There are also a few missing values.', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'A1:\\tb, a.\\r\\nA2:\\tcontinuous.\\r\\nA3:\\tcontinuous.\\r\\nA4:\\tu, y, l, t.\\r\\nA5:\\tg, p, gg.\\r\\nA6:\\tc, d, cc, i, j, k, m, r, q, w, x, e, aa, ff.\\r\\nA7:\\tv, h, bb, j, n, z, dd, ff, o.\\r\\nA8:\\tcontinuous.\\r\\nA9:\\tt, f.\\r\\nA10:\\tt, f.\\r\\nA11:\\tcontinuous.\\r\\nA12:\\tt, f.\\r\\nA13:\\tg, p, s.\\r\\nA14:\\tcontinuous.\\r\\nA15:\\tcontinuous.\\r\\nA16: +,-         (class attribute)', 'citation': None}}\n",
            "   name     role         type demographic description units missing_values\n",
            "0   A16   Target  Categorical        None        None  None             no\n",
            "1   A15  Feature   Continuous        None        None  None             no\n",
            "2   A14  Feature   Continuous        None        None  None            yes\n",
            "3   A13  Feature  Categorical        None        None  None             no\n",
            "4   A12  Feature  Categorical        None        None  None             no\n",
            "5   A11  Feature   Continuous        None        None  None             no\n",
            "6   A10  Feature  Categorical        None        None  None             no\n",
            "7    A9  Feature  Categorical        None        None  None             no\n",
            "8    A8  Feature   Continuous        None        None  None             no\n",
            "9    A7  Feature  Categorical        None        None  None            yes\n",
            "10   A6  Feature  Categorical        None        None  None            yes\n",
            "11   A5  Feature  Categorical        None        None  None            yes\n",
            "12   A4  Feature  Categorical        None        None  None            yes\n",
            "13   A3  Feature   Continuous        None        None  None             no\n",
            "14   A2  Feature   Continuous        None        None  None            yes\n",
            "15   A1  Feature  Categorical        None        None  None            yes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stable_attributes = ['A14','A11','A5', 'A4', 'A3']\n",
        "flexible_attributes = list(credit_approval.drop(columns=['A16','A14','A11','A5', 'A4', 'A3']).columns)\n",
        "decision_attributes = ['A16']\n",
        "\n",
        "print(\"Stable Attributes:\",stable_attributes)\n",
        "print(\"Flexible Attributes:\",flexible_attributes)\n",
        "print(\"Decision Attributes:\",decision_attributes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWTzHGzMQLm_",
        "outputId": "4dd76872-6e50-4130-a228-3c9cb8815b76"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stable Attributes: ['A14', 'A11', 'A5', 'A4', 'A3']\n",
            "Flexible Attributes: ['A15', 'A13', 'A12', 'A10', 'A9', 'A8', 'A7', 'A6', 'A2', 'A1']\n",
            "Decision Attributes: ['A16']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ActionRulesDiscovery:\n",
        "    def __init__(self, data, stable_attrs, flexible_attrs, decision_attr):\n",
        "        self.data = data\n",
        "        self.stable_attrs = stable_attrs\n",
        "        self.flexible_attrs = flexible_attrs\n",
        "        self.decision_attr = decision_attr\n",
        "        self.rules = []\n",
        "        self.action_rules = []\n",
        "\n",
        "    def extract_classification_rules(self, min_support=0.1, min_confidence=0.4):\n",
        "        \"\"\"Extract classification rules for each decision class\n",
        "           stores rules in rules\n",
        "        \"\"\"\n",
        "        decision_values = self.data[self.decision_attr].unique()\n",
        "\n",
        "        for decision_value in decision_values:\n",
        "            target_records = self.data[self.data[self.decision_attr] == decision_value]\n",
        "\n",
        "            for attr in self.flexible_attrs + self.stable_attrs:\n",
        "                value_counts = target_records[attr].value_counts()\n",
        "                support = value_counts / len(self.data) #calculate support\n",
        "                valid_values = support[support >= min_support]\n",
        "\n",
        "                for value in valid_values.index:\n",
        "                    total_with_value = len(self.data[self.data[attr] == value]) #Calculates the number of records in the whole dataset that have the current attribute value\n",
        "                    support_count = len(target_records[target_records[attr] == value]) #Counts the number of target records (for the current decision value) that have the attribute equal to the current value\n",
        "                    confidence = support_count / total_with_value #Calculate confidence\n",
        "\n",
        "                    if confidence >= min_confidence:\n",
        "                        # Calculate lift\n",
        "                        antecedent_support = len(self.data[self.data[attr] == value]) / len(self.data)\n",
        "                        consequent_support = len(self.data[self.data[self.decision_attr] == decision_value]) / len(self.data)\n",
        "                        lift = support[value] / (antecedent_support * consequent_support)\n",
        "\n",
        "                        rule = {\n",
        "                            'conditions': [(attr, value)],\n",
        "                            'decision': decision_value,\n",
        "                            'support': support[value],\n",
        "                            'confidence': confidence,\n",
        "                            'lift': lift\n",
        "                        }\n",
        "                        self.rules.append(rule)\n",
        "\n",
        "    def build_d_tree(self, decision_value):\n",
        "        \"\"\"Grouping rules into equivalence classes based on stable attributes \"\"\"\n",
        "        d_tree = defaultdict(list)\n",
        "        for rule in self.rules:\n",
        "            if rule['decision'] == decision_value:\n",
        "                # Extract stable attribute conditions\n",
        "                stable_conditions = tuple(\n",
        "                    (attr, val) for (attr, val) in rule['conditions']\n",
        "                    if attr in self.stable_attrs\n",
        "                )\n",
        "                d_tree[stable_conditions].append(rule)\n",
        "        return d_tree\n",
        "\n",
        "    def generate_action_rules(self, desired_effect):\n",
        "        \"\"\"Generate action rules using equivalence classes \"\"\"\n",
        "\n",
        "        # Get all decision values except the desired one\n",
        "        decision_values = self.data[self.decision_attr].unique()\n",
        "        source_decisions = [dv for dv in decision_values if dv != desired_effect]\n",
        "\n",
        "        # Build target d-tree (desired effect)\n",
        "        target_d_tree = self.build_d_tree(desired_effect)\n",
        "\n",
        "        for source_decision in source_decisions:\n",
        "            # Build source d-tree (current undesired class)\n",
        "            source_d_tree = self.build_d_tree(source_decision)\n",
        "\n",
        "            # Compare equivalence classes between source and target\n",
        "            for stable_key in source_d_tree:\n",
        "                if stable_key in target_d_tree:\n",
        "                    for source_rule in source_d_tree[stable_key]:\n",
        "                        for target_rule in target_d_tree[stable_key]:\n",
        "                            action_rule = self._compare_rules(source_rule, target_rule)\n",
        "                            if action_rule:\n",
        "                                self.action_rules.append(action_rule)\n",
        "\n",
        "    def _compare_rules(self, source_rule, target_rule):\n",
        "        \"\"\"Compare two rules to generate an action rule.\"\"\"\n",
        "        for attr, value in source_rule['conditions']:\n",
        "            if attr in self.stable_attrs:\n",
        "                return None\n",
        "                # target_value = next((v for a, v in target_rule['conditions'] if a == attr), None)\n",
        "                # if target_value and value != target_value:\n",
        "                #     return None\n",
        "\n",
        "        actions = []\n",
        "        for attr, source_value in source_rule['conditions']:\n",
        "            if attr in self.flexible_attrs:\n",
        "                target_value = next((v for a, v in target_rule['conditions'] if a == attr), None)\n",
        "                if target_value and source_value != target_value:\n",
        "                    actions.append((attr, f\"{source_value} → {target_value}\"))\n",
        "\n",
        "        if actions:\n",
        "            return {\n",
        "                'actions': actions,\n",
        "                'source_decision': source_rule['decision'],\n",
        "                'target_decision': target_rule['decision'],\n",
        "                'support': min(source_rule['support'], target_rule['support']),\n",
        "                'confidence': min(source_rule['confidence'], target_rule['confidence']),\n",
        "                'lift': min(source_rule['lift'], target_rule['lift'])\n",
        "            }\n",
        "        return None"
      ],
      "metadata": {
        "id": "WjOq4wzePezH"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_action_rules(action_rules, original_data, confidence_threshold=0.1):\n",
        "    \"\"\"\n",
        "    Applies action rules to transform the dataset by modifying flexible attributes and decision values.\n",
        "\n",
        "    Args:\n",
        "        action_rules (list): List of action rules (dictionaries with 'actions', 'confidence', etc.)\n",
        "        original_data (pd.DataFrame): Original dataset to transform\n",
        "        confidence_threshold (float): Minimum confidence required to apply a rule\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Transformed dataset with applied changes\n",
        "    \"\"\"\n",
        "    transformed_data = original_data.copy()\n",
        "    modified_records = set()  # Track records that have already been modified\n",
        "\n",
        "    for rule in action_rules:\n",
        "        if rule['confidence'] >= confidence_threshold:\n",
        "            # Build mask for records matching ALL action source values AND source decision\n",
        "            mask = pd.Series(True, index=transformed_data.index)\n",
        "            mask &= (transformed_data['A16'] == rule['source_decision'])\n",
        "\n",
        "            # Prepare source→target mappings for each attribute in the action\n",
        "            action_changes = []\n",
        "            for attr, value_change in rule['actions']:\n",
        "                source_val, target_val = value_change.split(' → ')\n",
        "                mask &= (transformed_data[attr] == source_val)\n",
        "                action_changes.append((attr, target_val))\n",
        "\n",
        "            # Apply changes to matching records that haven't been modified yet\n",
        "            for record_idx in transformed_data[mask].index:\n",
        "                if record_idx not in modified_records:\n",
        "                    # Update flexible attributes\n",
        "                    for attr, target_val in action_changes:\n",
        "                        transformed_data.at[record_idx, attr] = target_val\n",
        "                    # Update decision attribute\n",
        "                    transformed_data.at[record_idx, 'A16'] = rule['target_decision']\n",
        "                    modified_records.add(record_idx)\n",
        "\n",
        "    return transformed_data"
      ],
      "metadata": {
        "id": "Wx7VBvZuD-gJ"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_evaluate_model(original_data, transformed_data):\n",
        "    \"\"\"Train and evaluate Random Forest models on both original and transformed data.\"\"\"\n",
        "    # Split original data\n",
        "    X = original_data.drop('A16', axis=1)\n",
        "    y = original_data['A16']\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Split transformed data\n",
        "    X_t = transformed_data.drop('A16', axis=1)\n",
        "    y_t = transformed_data['A16']\n",
        "    X_t_train, X_t_test, y_t_train, y_t_test = train_test_split(X_t, y_t, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Initialize encoder and model\n",
        "    encoder = ce.OneHotEncoder(handle_unknown='ignore', use_cat_names=True)\n",
        "    clf = RandomForestClassifier(max_depth=24, random_state=0)\n",
        "\n",
        "    # Train and evaluate on original data\n",
        "    X_train_encoded = encoder.fit_transform(X_train)\n",
        "    X_test_encoded = encoder.transform(X_test)\n",
        "\n",
        "    X_t_encoded = encoder.transform(X_t)\n",
        "    X_encoded = encoder.transform(X)\n",
        "    X_t_test_encoded = encoder.transform(X_t_test)\n",
        "\n",
        "    clf.fit(X_train_encoded, y_train)\n",
        "\n",
        "    # Confusion matrix for actual values of original dataset (entire dataset)\n",
        "    original_confusion_actual = confusion_matrix(y, y)\n",
        "    transformed_confusion_actual = confusion_matrix(y_t, y_t)\n",
        "\n",
        "    # Accuracy and confusion for original data after predictions\n",
        "    y_pred = clf.predict(X_test_encoded)\n",
        "    original_accuracy = accuracy_score(y_test, y_pred)\n",
        "    original_confusion_pred = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    # Accuracy and confusion for transformed data after predictions\n",
        "    y_t_pred = clf.predict(X_t_test_encoded)\n",
        "    transformed_accuracy = accuracy_score(y_test, y_t_pred)\n",
        "    transformed_confusion_pred = confusion_matrix(y_test, y_t_pred)\n",
        "\n",
        "    # Full accuracy and confusion for original and transformed data (based on full dataset)\n",
        "    full_t_accuracy = accuracy_score(y, clf.predict(X_t_encoded))\n",
        "    full_t_confusion = confusion_matrix(y, clf.predict(X_t_encoded))\n",
        "\n",
        "    full_accuracy = accuracy_score(y, clf.predict(X_encoded))\n",
        "    full_confusion = confusion_matrix(y, clf.predict(X_encoded))\n",
        "\n",
        "    return {\n",
        "        'original': {'accuracy': original_accuracy, 'confusion_matrix': original_confusion_pred},\n",
        "        'transformed': {'accuracy': transformed_accuracy, 'confusion_matrix': transformed_confusion_pred},\n",
        "        'full transformed': {'accuracy': full_accuracy, 'confusion_matrix': full_confusion},\n",
        "        'full original': {'accuracy': full_t_accuracy, 'confusion_matrix': full_t_confusion},\n",
        "        'decision_distribution': {'original': y.value_counts(), 'transformed': y_t.value_counts()}\n",
        "    }\n"
      ],
      "metadata": {
        "id": "lFr5FpcNPTQv"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_action_rules(action_rules):\n",
        "    \"\"\"\n",
        "    Print action rules in a clear, formatted way.\n",
        "\n",
        "    Args:\n",
        "        action_rules: List of action rule dictionaries from ActionRulesDiscovery\n",
        "    \"\"\"\n",
        "    if not action_rules:\n",
        "        print(\"No action rules found.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n=== Action Rules ===\")\n",
        "    for i, rule in enumerate(action_rules, 1):\n",
        "        print(f\"\\nRule {i}:\")\n",
        "        print(\"Actions:\")\n",
        "        for attr, change in rule['actions']:\n",
        "            print(f\"  • {attr}: {change}\")\n",
        "\n",
        "        print(f\"Source Decision: {rule['source_decision']}\")\n",
        "        print(f\"Target Decision: {rule['target_decision']}\")\n",
        "        print(f\"Support: {rule['support']:.3f}\")\n",
        "        print(f\"Confidence: {rule['confidence']:.3f}\")\n",
        "        print(f\"Lift: {rule['lift']:.3f}\")\n",
        "        print(\"-\" * 40)"
      ],
      "metadata": {
        "id": "PYjg89E-Pv_n"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize class\n",
        "action_discovery = ActionRulesDiscovery(\n",
        "    data=credit_approval,\n",
        "    stable_attrs=stable_attributes,\n",
        "    flexible_attrs=flexible_attributes,\n",
        "    decision_attr='A16'\n",
        ")\n",
        "\n",
        "# Generate action rules\n",
        "action_discovery.extract_classification_rules()\n",
        "action_discovery.generate_action_rules(desired_effect='+')\n",
        "\n",
        "print_action_rules(action_discovery.action_rules)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umSIgRzBEiil",
        "outputId": "b9d9f4df-16bb-42bd-d0f1-0e86982e8069"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Action Rules ===\n",
            "\n",
            "Rule 1:\n",
            "Actions:\n",
            "  • A12: f → t\n",
            "Source Decision: -\n",
            "Target Decision: +\n",
            "Support: 0.212\n",
            "Confidence: 0.462\n",
            "Lift: 1.026\n",
            "----------------------------------------\n",
            "\n",
            "Rule 2:\n",
            "Actions:\n",
            "  • A12: t → f\n",
            "Source Decision: -\n",
            "Target Decision: +\n",
            "Support: 0.233\n",
            "Confidence: 0.430\n",
            "Lift: 0.968\n",
            "----------------------------------------\n",
            "\n",
            "Rule 3:\n",
            "Actions:\n",
            "  • A10: f → t\n",
            "Source Decision: -\n",
            "Target Decision: +\n",
            "Support: 0.303\n",
            "Confidence: 0.708\n",
            "Lift: 1.355\n",
            "----------------------------------------\n",
            "\n",
            "Rule 4:\n",
            "Actions:\n",
            "  • A9: f → t\n",
            "Source Decision: -\n",
            "Target Decision: +\n",
            "Support: 0.412\n",
            "Confidence: 0.787\n",
            "Lift: 1.676\n",
            "----------------------------------------\n",
            "\n",
            "Rule 5:\n",
            "Actions:\n",
            "  • A7: v → h\n",
            "Source Decision: -\n",
            "Target Decision: +\n",
            "Support: 0.126\n",
            "Confidence: 0.576\n",
            "Lift: 1.038\n",
            "----------------------------------------\n",
            "\n",
            "Rule 6:\n",
            "Actions:\n",
            "  • A1: b → a\n",
            "Source Decision: -\n",
            "Target Decision: +\n",
            "Support: 0.142\n",
            "Confidence: 0.467\n",
            "Lift: 1.009\n",
            "----------------------------------------\n",
            "\n",
            "Rule 7:\n",
            "Actions:\n",
            "  • A1: a → b\n",
            "Source Decision: -\n",
            "Target Decision: +\n",
            "Support: 0.162\n",
            "Confidence: 0.440\n",
            "Lift: 0.961\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_dataset = apply_action_rules(\n",
        "    action_rules=action_discovery.action_rules,\n",
        "    original_data=credit_approval,\n",
        "    confidence_threshold=0.75\n",
        ")\n",
        "\n",
        "results = train_evaluate_model(credit_approval, transformed_dataset)\n",
        "\n",
        "# decsision values count\n",
        "print(\"Decision distribution in original data:\\n\", results['decision_distribution']['original'])\n",
        "print(\"Decision distribution in transformed data:\\n\", results['decision_distribution']['transformed'])\n",
        "\n",
        "print(\"Original data accuracy:\", results['original']['accuracy'])\n",
        "# print(\"Transformed data accuracy:\", results['transformed']['accuracy'])\n",
        "\n",
        "print(\"Original data confusion_matrix:\\n\", results['original']['confusion_matrix'])\n",
        "print(\"Transformed data confusion_matrix:\\n\", results['transformed']['confusion_matrix'])\n",
        "\n",
        "# print(\"Full orginal data accuracy:\", results['full transformed']['accuracy'])\n",
        "print(\"Full orginal data confusion_matrix:\\n\", results['full transformed']['confusion_matrix'])\n",
        "\n",
        "# print(\"Full transformed data accuracy:\", results['full original']['accuracy'])\n",
        "print(\"Full transformed data confusion_matrix:\\n\", results['full original']['confusion_matrix'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXpKbXaEPy_6",
        "outputId": "a63f6437-2c4c-47a5-ee2c-dba7527b5689"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision distribution in original data:\n",
            " A16\n",
            "-    383\n",
            "+    307\n",
            "Name: count, dtype: int64\n",
            "Decision distribution in transformed data:\n",
            " A16\n",
            "+    613\n",
            "-     77\n",
            "Name: count, dtype: int64\n",
            "Original data accuracy: 0.8623188405797102\n",
            "Original data confusion_matrix:\n",
            " [[59 11]\n",
            " [ 8 60]]\n",
            "Transformed data confusion_matrix:\n",
            " [[59 11]\n",
            " [37 31]]\n",
            "Full orginal data confusion_matrix:\n",
            " [[296  11]\n",
            " [  8 375]]\n",
            "Full transformed data confusion_matrix:\n",
            " [[296  11]\n",
            " [164 219]]\n"
          ]
        }
      ]
    }
  ]
}
